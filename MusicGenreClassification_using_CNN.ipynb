{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu2CRQVc5Vnz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ0auZw1xejK",
        "outputId": "93d9db94-beda-4864-e67b-f1600cd17a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQOaks54_kSM"
      },
      "source": [
        "# **CNN2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foiYbYR1-yEw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEg1OLTU_sNz"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 60\n",
        "NUM_CLASSES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmYm6hH-_uzH"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation and Normalization\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMUKDOz1_xJl",
        "outputId": "08bf99ff-0424-4f56-d96c-886ebff96d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 10 classes.\n",
            "Found 199 images belonging to 10 classes.\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Music Genre Classification/images_original',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Music Genre Classification/images_original',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ych61JlrACne"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dropout\n",
        "\n",
        "# Define the CNN model with Dropout regularization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),  # Adding Dropout regularization\n",
        "   (6 Conv2D4, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),  # Adding Dropout regularization\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),  # Adding Dropout regularization\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Adding Dropout regularization\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzR-NOsSAITY"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei9e6NDjALUe",
        "outputId": "bef79b30-e99c-46c6-f2fa-9a0b529b3594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "25/25 [==============================] - 359s 14s/step - loss: 2.7230 - accuracy: 0.0950 - val_loss: 2.3021 - val_accuracy: 0.1094\n",
            "Epoch 2/60\n",
            "25/25 [==============================] - 55s 2s/step - loss: 2.3006 - accuracy: 0.1075 - val_loss: 2.2906 - val_accuracy: 0.1250\n",
            "Epoch 3/60\n",
            "25/25 [==============================] - 47s 2s/step - loss: 2.2812 - accuracy: 0.1088 - val_loss: 2.2744 - val_accuracy: 0.2135\n",
            "Epoch 4/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 2.2436 - accuracy: 0.1713 - val_loss: 2.2282 - val_accuracy: 0.1927\n",
            "Epoch 5/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 2.2228 - accuracy: 0.1850 - val_loss: 2.1919 - val_accuracy: 0.2292\n",
            "Epoch 6/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 2.1915 - accuracy: 0.2013 - val_loss: 2.1746 - val_accuracy: 0.2135\n",
            "Epoch 7/60\n",
            "25/25 [==============================] - 48s 2s/step - loss: 2.1836 - accuracy: 0.2087 - val_loss: 2.1223 - val_accuracy: 0.2448\n",
            "Epoch 8/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 2.1740 - accuracy: 0.2037 - val_loss: 2.1441 - val_accuracy: 0.1979\n",
            "Epoch 9/60\n",
            "25/25 [==============================] - 47s 2s/step - loss: 2.1183 - accuracy: 0.2338 - val_loss: 2.0530 - val_accuracy: 0.2708\n",
            "Epoch 10/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 2.0689 - accuracy: 0.2488 - val_loss: 2.0136 - val_accuracy: 0.3021\n",
            "Epoch 11/60\n",
            "25/25 [==============================] - 48s 2s/step - loss: 2.0822 - accuracy: 0.2350 - val_loss: 2.1222 - val_accuracy: 0.1927\n",
            "Epoch 12/60\n",
            "25/25 [==============================] - 53s 2s/step - loss: 2.0248 - accuracy: 0.2537 - val_loss: 1.9091 - val_accuracy: 0.2812\n",
            "Epoch 13/60\n",
            "25/25 [==============================] - 50s 2s/step - loss: 1.9621 - accuracy: 0.2875 - val_loss: 1.9521 - val_accuracy: 0.2708\n",
            "Epoch 14/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.9249 - accuracy: 0.2937 - val_loss: 1.8087 - val_accuracy: 0.3594\n",
            "Epoch 15/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.8640 - accuracy: 0.3363 - val_loss: 1.8604 - val_accuracy: 0.3594\n",
            "Epoch 16/60\n",
            "25/25 [==============================] - 47s 2s/step - loss: 1.8304 - accuracy: 0.3225 - val_loss: 1.7504 - val_accuracy: 0.3750\n",
            "Epoch 17/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 1.7773 - accuracy: 0.3225 - val_loss: 1.7358 - val_accuracy: 0.3438\n",
            "Epoch 18/60\n",
            "25/25 [==============================] - 47s 2s/step - loss: 1.7056 - accuracy: 0.3812 - val_loss: 1.8103 - val_accuracy: 0.3229\n",
            "Epoch 19/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 1.7005 - accuracy: 0.3663 - val_loss: 1.7252 - val_accuracy: 0.3438\n",
            "Epoch 20/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.6548 - accuracy: 0.3925 - val_loss: 1.6483 - val_accuracy: 0.3958\n",
            "Epoch 21/60\n",
            "25/25 [==============================] - 52s 2s/step - loss: 1.6416 - accuracy: 0.3800 - val_loss: 1.7057 - val_accuracy: 0.3854\n",
            "Epoch 22/60\n",
            "25/25 [==============================] - 47s 2s/step - loss: 1.6030 - accuracy: 0.4137 - val_loss: 1.5775 - val_accuracy: 0.4167\n",
            "Epoch 23/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.5774 - accuracy: 0.4225 - val_loss: 1.5875 - val_accuracy: 0.4531\n",
            "Epoch 24/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.5792 - accuracy: 0.4175 - val_loss: 1.7677 - val_accuracy: 0.3698\n",
            "Epoch 25/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.5705 - accuracy: 0.4288 - val_loss: 1.6110 - val_accuracy: 0.4167\n",
            "Epoch 26/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.4993 - accuracy: 0.4675 - val_loss: 1.6251 - val_accuracy: 0.4219\n",
            "Epoch 27/60\n",
            "25/25 [==============================] - 44s 2s/step - loss: 1.4981 - accuracy: 0.4575 - val_loss: 1.5629 - val_accuracy: 0.3906\n",
            "Epoch 28/60\n",
            "25/25 [==============================] - 49s 2s/step - loss: 1.4787 - accuracy: 0.4525 - val_loss: 1.5294 - val_accuracy: 0.4531\n",
            "Epoch 29/60\n",
            "25/25 [==============================] - 47s 2s/step - loss: 1.4198 - accuracy: 0.4863 - val_loss: 1.5189 - val_accuracy: 0.4219\n",
            "Epoch 30/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 1.3752 - accuracy: 0.4938 - val_loss: 1.4748 - val_accuracy: 0.4740\n",
            "Epoch 31/60\n",
            "25/25 [==============================] - 44s 2s/step - loss: 1.3723 - accuracy: 0.4950 - val_loss: 1.6429 - val_accuracy: 0.4219\n",
            "Epoch 32/60\n",
            "25/25 [==============================] - 53s 2s/step - loss: 1.4045 - accuracy: 0.4913 - val_loss: 1.4675 - val_accuracy: 0.4740\n",
            "Epoch 33/60\n",
            "25/25 [==============================] - 48s 2s/step - loss: 1.3333 - accuracy: 0.5125 - val_loss: 1.5362 - val_accuracy: 0.4635\n",
            "Epoch 34/60\n",
            "25/25 [==============================] - 44s 2s/step - loss: 1.3616 - accuracy: 0.5163 - val_loss: 1.6015 - val_accuracy: 0.4531\n",
            "Epoch 35/60\n",
            "25/25 [==============================] - 49s 2s/step - loss: 1.3378 - accuracy: 0.5138 - val_loss: 1.4471 - val_accuracy: 0.4740\n",
            "Epoch 36/60\n",
            "25/25 [==============================] - 44s 2s/step - loss: 1.3213 - accuracy: 0.5188 - val_loss: 1.5002 - val_accuracy: 0.5104\n",
            "Epoch 37/60\n",
            "25/25 [==============================] - 50s 2s/step - loss: 1.2864 - accuracy: 0.5150 - val_loss: 1.4769 - val_accuracy: 0.4635\n",
            "Epoch 38/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.3138 - accuracy: 0.5312 - val_loss: 1.3905 - val_accuracy: 0.5000\n",
            "Epoch 39/60\n",
            "25/25 [==============================] - 52s 2s/step - loss: 1.2804 - accuracy: 0.5462 - val_loss: 1.4803 - val_accuracy: 0.4479\n",
            "Epoch 40/60\n",
            "25/25 [==============================] - 50s 2s/step - loss: 1.2599 - accuracy: 0.5450 - val_loss: 1.4982 - val_accuracy: 0.4844\n",
            "Epoch 41/60\n",
            "25/25 [==============================] - 56s 2s/step - loss: 1.2724 - accuracy: 0.5412 - val_loss: 1.4458 - val_accuracy: 0.4688\n",
            "Epoch 42/60\n",
            "25/25 [==============================] - 52s 2s/step - loss: 1.2531 - accuracy: 0.5500 - val_loss: 1.4834 - val_accuracy: 0.4740\n",
            "Epoch 43/60\n",
            "25/25 [==============================] - 51s 2s/step - loss: 1.2762 - accuracy: 0.5288 - val_loss: 1.6046 - val_accuracy: 0.4062\n",
            "Epoch 44/60\n",
            "25/25 [==============================] - 51s 2s/step - loss: 1.2268 - accuracy: 0.5600 - val_loss: 1.4813 - val_accuracy: 0.4583\n",
            "Epoch 45/60\n",
            "25/25 [==============================] - 51s 2s/step - loss: 1.2909 - accuracy: 0.5450 - val_loss: 1.4804 - val_accuracy: 0.4427\n",
            "Epoch 46/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.2018 - accuracy: 0.5412 - val_loss: 1.4701 - val_accuracy: 0.4688\n",
            "Epoch 47/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 1.1515 - accuracy: 0.5587 - val_loss: 1.4422 - val_accuracy: 0.4740\n",
            "Epoch 48/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.2007 - accuracy: 0.5663 - val_loss: 1.4970 - val_accuracy: 0.4635\n",
            "Epoch 49/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.1756 - accuracy: 0.5913 - val_loss: 1.4415 - val_accuracy: 0.4635\n",
            "Epoch 50/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 1.2142 - accuracy: 0.5688 - val_loss: 1.5173 - val_accuracy: 0.4479\n",
            "Epoch 51/60\n",
            "25/25 [==============================] - 44s 2s/step - loss: 1.0979 - accuracy: 0.5938 - val_loss: 1.5636 - val_accuracy: 0.5052\n",
            "Epoch 52/60\n",
            "25/25 [==============================] - 49s 2s/step - loss: 1.1380 - accuracy: 0.6025 - val_loss: 1.4976 - val_accuracy: 0.5000\n",
            "Epoch 53/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.1027 - accuracy: 0.6087 - val_loss: 1.4806 - val_accuracy: 0.4792\n",
            "Epoch 54/60\n",
            "25/25 [==============================] - 44s 2s/step - loss: 1.1443 - accuracy: 0.5888 - val_loss: 1.5150 - val_accuracy: 0.5208\n",
            "Epoch 55/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.1035 - accuracy: 0.5962 - val_loss: 1.5298 - val_accuracy: 0.4948\n",
            "Epoch 56/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.1317 - accuracy: 0.5913 - val_loss: 1.3920 - val_accuracy: 0.5000\n",
            "Epoch 57/60\n",
            "25/25 [==============================] - 46s 2s/step - loss: 1.1457 - accuracy: 0.5838 - val_loss: 1.4523 - val_accuracy: 0.4740\n",
            "Epoch 58/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 1.0759 - accuracy: 0.6112 - val_loss: 1.5844 - val_accuracy: 0.4792\n",
            "Epoch 59/60\n",
            "25/25 [==============================] - 47s 2s/step - loss: 1.1054 - accuracy: 0.6087 - val_loss: 1.4765 - val_accuracy: 0.4792\n",
            "Epoch 60/60\n",
            "25/25 [==============================] - 45s 2s/step - loss: 1.0630 - accuracy: 0.6112 - val_loss: 1.4171 - val_accuracy: 0.5052\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4p8ihH7ANxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1efff881-e5df-45ad-f8c2-b8ea02689e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "model.save('CNN.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjcM5JFAQu36"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import librosa\n",
        "from scipy import signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained CNN model\n",
        "model = load_model('CNN.h5')\n",
        "\n",
        "# Define constants for preprocessing spectrogram images\n",
        "IMAGE_SIZE = (128, 128)"
      ],
      "metadata": {
        "id": "6GzGmn49jeEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# Load the trained CNN model\n",
        "model = load_model('CNN.h5')\n",
        "\n",
        "# Define constants for preprocessing images\n",
        "IMAGE_SIZE = (128, 128)\n",
        "\n",
        "# Prediction function\n",
        "def predict_genre_from_image(image_file_path):\n",
        "    try:\n",
        "        # Step 1: Load the image and resize it to match the input size of the CNN model\n",
        "        image = load_img(image_file_path, target_size=IMAGE_SIZE)\n",
        "        # Convert image to array\n",
        "        image_array = img_to_array(image)\n",
        "        # Expand dimensions to create a batch\n",
        "        image_batch = np.expand_dims(image_array, axis=0)\n",
        "\n",
        "        # Step 2: Preprocess the image\n",
        "        preprocessed_image = preprocess_input(image_batch)\n",
        "\n",
        "        # Step 3: Use the trained model to predict the genre of the image\n",
        "        prediction = model.predict(preprocessed_image)\n",
        "\n",
        "        # Step 4: Decode the prediction and get the predicted genre\n",
        "        predicted_class_index = np.argmax(prediction)\n",
        "        genre_labels = [\n",
        "            'Jazz', 'Blues', 'Country', 'Disco', 'HipHop',\n",
        "            'Classical', 'Metal', 'Pop', 'Reggae', 'Rock'\n",
        "        ]\n",
        "        predicted_genre = genre_labels[predicted_class_index]\n",
        "\n",
        "        return predicted_genre\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred while predicting genre:\", e)\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "image_file_path = '/content/drive/MyDrive/Music Genre Classification/images_original/blues/blues00065.png'\n",
        "predicted_genre = predict_genre_from_image(image_file_path)\n",
        "print(\"Predicted genre:\", predicted_genre)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uavHj0Hcjhlf",
        "outputId": "1d38affb-36ec-4b52-9cc3-6cbabfafcc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 99ms/step\n",
            "Predicted genre: Classical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VSGMJ7FK6wuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}